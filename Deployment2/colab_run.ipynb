{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-3-1B-it RAG System — Google Colab\n",
    "\n",
    "**Vor dem Start:** Oben rechts auf `Laufzeit` → `Laufzeittyp ändern` → **T4 GPU** auswählen!\n",
    "\n",
    "Dann alle Zellen von oben nach unten ausführen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zelle 1: GPU prüfen\n",
    "import torch\n",
    "print('CUDA verfügbar:', torch.cuda.is_available())\n",
    "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'KEINE GPU — Laufzeittyp auf T4 setzen!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zelle 2: Abhängigkeiten installieren\n",
    "!pip install transformers accelerate bitsandbytes sentence-transformers chromadb fastapi\n",
    "!pip install --upgrade wandb -q\n",
    "uvicorn gradio pydantic-settings python-dotenv requests wandb --upgrade -q"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Zelle 3: GitHub Repository klonen\n!git clone https://github.com/SebastianKuehnrich/gemma3-rag-api.git\n%cd gemma3-rag-api/Deployment2\n!ls",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Zelle 4: HF Token aus Colab Secrets laden (Gemma ist ein gated model)\nfrom google.colab import userdata\nimport os\n\nhf_token = userdata.get('HF_TOKEN')\nos.environ['HF_TOKEN'] = hf_token\n\n# .env Datei schreiben damit pydantic-settings sie findet\nwith open('.env', 'w') as f:\n    f.write(f\"HF_TOKEN={hf_token}\\n\")\n    f.write(\"USE_4BIT_QUANTIZATION=True\\n\")\n    f.write(\"MAX_NEW_TOKENS=200\\n\")\nprint('.env geschrieben')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zelle 5: Datenbank initialisieren\n",
    "!python -m scripts.init_database"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zelle 6: FastAPI im Hintergrund starten\n",
    "import threading, subprocess, time, requests\n",
    "\n",
    "def run_fastapi():\n",
    "    subprocess.run(['uvicorn', 'app.main:app', '--host', '0.0.0.0', '--port', '8001'])\n",
    "\n",
    "thread = threading.Thread(target=run_fastapi, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "# Warten bis FastAPI bereit ist (Gemma laden dauert ~30-60s auf T4)\n",
    "print('Warte auf FastAPI + Gemma...')\n",
    "for i in range(120):\n",
    "    try:\n",
    "        r = requests.get('http://127.0.0.1:8001/health', timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            print(f'FastAPI bereit! Gemma geladen: {data[\"gemma_loaded\"]}')\n",
    "            print(f'Dokumente in DB: {data[\"num_documents\"]}')\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(2)\n",
    "    if i % 10 == 0:\n",
    "        print(f'  ... {i*2}s vergangen')\n",
    "else:\n",
    "    print('TIMEOUT — FastAPI hat nicht gestartet')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zelle 7: Schneller API-Test direkt in Python\n",
    "import requests, json\n",
    "\n",
    "response = requests.post(\n",
    "    'http://127.0.0.1:8001/query',\n",
    "    json={'query': 'Was ist Machine Learning?', 'top_k': 3, 'use_rag': True},\n",
    "    timeout=60\n",
    ")\n",
    "data = response.json()\n",
    "print('Antwort:', data['answer'])\n",
    "print('Generierungszeit:', data['generation_time_seconds'], 's')\n",
    "print('Kontext-Dokumente:', data['num_context_docs'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Zelle 8: Gradio UI mit öffentlicher URL starten\nimport ui.gradio_app as gradio_module\nfrom ui.gradio_app import demo\n\n# API URL auf lokalen FastAPI Server setzen\ngradio_module.API_URL = 'http://127.0.0.1:8001'\n\n# Gradio starten — share=True erzeugt automatisch eine öffentliche URL\ndemo.launch(\n    server_name='0.0.0.0',\n    server_port=7860,\n    share=True,\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## BONUS: Fine-Tuning mit wandb (optional)\n",
    "\n",
    "Diese Zelle startet das Fine-Tuning auf der T4 GPU. Dauert ~15-30 Minuten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Zelle 9 (OPTIONAL): wandb Login + Fine-Tuning starten\nfrom google.colab import userdata\nimport os\n\nwandb_key = userdata.get('WANDB_API_KEY')\nos.environ['WANDB_API_KEY'] = wandb_key\n\n# .env updaten\nwith open('.env', 'a') as f:\n    f.write(f\"WANDB_API_KEY={wandb_key}\\n\")\n\n!python fine_tune.py",
   "execution_count": null,
   "outputs": []
  }
 ]
}
